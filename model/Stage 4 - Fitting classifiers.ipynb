{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import concurrent.futures\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "RND_STATE = 100412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_name):\n",
    "    max_bytes = 2**31 - 1\n",
    "    bytes_in = bytearray(0)\n",
    "    input_size = os.path.getsize(file_name)\n",
    "    with open(file_name, 'rb') as f_in:\n",
    "        for _ in range(0, input_size, max_bytes):\n",
    "            bytes_in += f_in.read(max_bytes)\n",
    "    return pickle.loads(bytes_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(file_name, data_to_save):\n",
    "    n_bytes = 2**31\n",
    "    max_bytes = 2**31 - 1\n",
    "    bytes_out = pickle.dumps(data_to_save)\n",
    "    with open(file_name, 'w+b') as f_out:\n",
    "        for idx in range(0, n_bytes, max_bytes):\n",
    "            f_out.write(bytes_out[idx:idx+max_bytes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_preprocessing(data_df):  \n",
    "    data_info = data_df.copy()\n",
    "    data_info['previous_flight_delay'] = list(map(int, data_info['late_aircraft_delay'] > 30))\n",
    "    data_info = data_info.drop(['cancellation_code', 'cancelled', 'carrier_delay', 'dep_delay_new', 'late_aircraft_delay', 'nas_delay', 'security_delay', 'weather_delay', 'diverted', 'origin_city_name', 'dest_city_name'], axis = 1)\n",
    "    \n",
    "    data_info = data_info.drop(['snowfall', 'snow_depth', 'thunder', 'dust', 'haze', 'snow', 'fog', 'hail', 'damaging_wind'], axis = 1)\n",
    "    \n",
    "    data_info['crs_dep_time'] = list(map(int, working_df['crs_dep_time'].values / 100))    \n",
    "    return data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_airline_avg_delay(carrier):\n",
    "    return carrier_average_delays[carrier_average_delays['carrier'] == carrier]['carrier_delay'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_airline_delay_index(carrier):\n",
    "    return delay_info[delay_info['Carrier'] == carrier]['Delay index'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_df):\n",
    "    data_info = data_df.copy()\n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor(16) as pool:\n",
    "        data_info['airline_delay_index'] = list(pool.map(add_airline_delay_index, data_info['carrier'], chunksize=2_000))\n",
    "        data_info['airline_avg_delay'] = list(pool.map(add_airline_avg_delay, data_info['carrier'], chunksize=2_000))\n",
    "    data_info = pd.get_dummies(data_info, columns=['dest'])\n",
    "    data_info['day_of_year'] = (data_info['fl_date'] - data_info['fl_date'].min())  / np.timedelta64(1,'D')\n",
    "    \n",
    "    data_info['weekend'] = np.where(data_info['day_of_week'] >= 6, 1, 0)\n",
    "    \n",
    "    data_info = data_info.drop(['fl_date', 'fl_num', 'origin', 'tail_num', 'carrier'], axis=1)\n",
    "    return data_info, data_info.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'historical_data'\n",
    "DICT_FOLDER = 'dictionaries'\n",
    "WEATHER_FOLDER = 'weather_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_CLF = '../data/best_clf.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PICKLE = '../data/merged_data.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_file(DATA_PICKLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load_file(BEST_CLF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_group = data[['status', 'carrier']]\n",
    "airlines_group_num = airlines_group.groupby(['carrier']).size()\n",
    "airlines_group = data[['status', 'carrier']]\n",
    "airlines_group = airlines_group[(airlines_group['status'] != 'no_delay')]\n",
    "airlines_group_delays_num = airlines_group.groupby(['carrier']).size()\n",
    "delay_info = pd.DataFrame({'Carrier': np.unique(airlines_group.carrier.values), 'Number of flights': airlines_group_num.values, 'Number of delays': airlines_group_delays_num.values})\n",
    "delay_info['Delay index'] = delay_info['Number of delays'] / delay_info['Number of flights']\n",
    "carrier_average_delays = pd.DataFrame(data.groupby(['carrier'])['carrier_delay'].mean()).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = np.unique(data['origin'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c5375d558d4514973bcdfdfe403661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=319)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found array with 0 sample(s) (shape=(0, 13)) while a minimum of 1 is required.\n",
      "Found array with 0 sample(s) (shape=(0, 15)) while a minimum of 1 is required.\n",
      "Found array with 0 sample(s) (shape=(0, 13)) while a minimum of 1 is required.\n",
      "Found array with 0 sample(s) (shape=(0, 13)) while a minimum of 1 is required.\n",
      "Found array with 0 sample(s) (shape=(0, 13)) while a minimum of 1 is required.\n",
      "Found array with 0 sample(s) (shape=(0, 14)) while a minimum of 1 is required.\n",
      "Found array with 0 sample(s) (shape=(0, 14)) while a minimum of 1 is required.\n",
      "Found array with 0 sample(s) (shape=(0, 13)) while a minimum of 1 is required.\n",
      "Found array with 0 sample(s) (shape=(0, 13)) while a minimum of 1 is required.\n",
      "Found array with 0 sample(s) (shape=(0, 15)) while a minimum of 1 is required.\n",
      "Found array with 0 sample(s) (shape=(0, 13)) while a minimum of 1 is required.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for airport in log_progress(airports, every=1):\n",
    "    try:\n",
    "        working_df = data[data['origin'] == airport]\n",
    "        \n",
    "        working_df = additional_preprocessing(working_df)\n",
    "        working_df, values_dict = process_data(working_df)\n",
    "        \n",
    "        working_df = working_df.dropna()\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(working_df.loc[:, working_df.columns != 'status'], working_df['status'], test_size = 0.2, random_state = RND_STATE)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        save_file('../data/clfs/' + airport + '.data', clf)\n",
    "        save_file('../data/values_dicts/' + airport + '.data', values_dict)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        f1_micro = f1_score(y_pred, y_test, average='macro')\n",
    "        f1_macro = f1_score(y_pred, y_test, average='weighted')\n",
    "        results.append({'airport': airport, 'f1_micro': f1_micro, 'f1_weighted': f1_macro})\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flight infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_info = data.copy()\n",
    "flight_info = flight_info.drop_duplicates(subset=['carrier', 'fl_num', 'origin'])\n",
    "flight_info = flight_info[['carrier', 'crs_dep_time', 'crs_elapsed_time', 'origin', 'origin_city_name', 'dest', 'dest_city_name', 'fl_num']]\n",
    "flight_info['crs_dep_time'] = list(map(int, flight_info['crs_dep_time'].values / 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file('../data/flight_info.data', flight_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_info = pd.DataFrame({'Carrier': np.unique(airlines_group.carrier.values), 'Number of flights': airlines_group_num.values, 'Number of delays': airlines_group_delays_num.values})\n",
    "delay_info['Delay index'] = delay_info['Number of delays'] / delay_info['Number of flights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file('../data/delays.data', delay_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_average_delays = pd.DataFrame(data.groupby(['carrier'])['carrier_delay'].mean()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file('../data/avg_delays.data', carrier_average_delays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
